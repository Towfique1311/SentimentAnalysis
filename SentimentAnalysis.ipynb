{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb ",
      "provenance": [],
      "authorship_tag": "ABX9TyPEkU2WWHpSvz5sJ0psahxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Towfique1311/SentimentAnalysis/blob/main/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#X_train and Y_tain and X_test is our Dataset. We can add more data if needed"
      ],
      "metadata": {
        "id": "h6bZ10sMRfC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Training data\n",
        " X_train = [\"This was awesome and tasty food\",\n",
        "\"Great food! I like it a lot\",\n",
        "\"Happy Ending! awesome acting by the actress\",\n",
        "\"Adore it! it was truly great experience\",\n",
        "\"Bad not upto the hype\",\n",
        "\"Could have been better\",\n",
        "\"Surely a Disapointing meal\"]\n",
        "\n",
        "Y_train = [1,1,1,1,0,0,0] # 1 is Positive, 0 is Negative class\n",
        "\n",
        "#testing data\n",
        "X_test = [\"It was awesome and I loved the meal and now I am happy\",\" I saw the movie  and it was bad\"]"
      ],
      "metadata": {
        "id": "mBbutc_6nbMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this part we will be cleaning our data. Such as tokenization etc."
      ],
      "metadata": {
        "id": "yk1tTK6ogtZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we are importing our necessary libraries for data cleaning \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "NYmjEyNvgrRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y6MK86KnY9C",
        "outputId": "1204b18d-b34c-48fc-a1c7-3a72a2de88a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#Importing nltk and downloading the stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating an object for tokenizer\n",
        "#we are passing words for regular expression tokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "#we are selecting stopwords for english\n",
        "eng_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "#we are creating porterstemmer\n",
        "ps = PorterStemmer()\n"
      ],
      "metadata": {
        "id": "q8YPw-4zh0Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we are creating a function for clening data\n",
        "def getCleanedText(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  #now we are performing tokenization\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  new_tokens =  [token for token in tokens if token not in eng_stopwords] # we are combinign tokenizer and stopwords removal\n",
        "  \n",
        "  #here we are doing stemming\n",
        "  stemmed_tokens =  [ps.stem(tokens) for tokens in new_tokens]\n",
        "\n",
        "  clean_text = \" \".join(stemmed_tokens)\n",
        "  return clean_text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xF9FX5zkiUzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the above defined function, we are cleaning our training and testing data "
      ],
      "metadata": {
        "id": "TF56a_drlnN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_clean = [getCleanedText(i) for i in X_train]\n",
        "Xt_clean = [getCleanedText(i) for i in X_test]\n",
        "            \n"
      ],
      "metadata": {
        "id": "9EvZ-yX9lft1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYMP7us-r04I",
        "outputId": "6e1555aa-0b04-46f4-bb74-00f6c0041449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['awesom tasti food',\n",
              " 'great food like lot',\n",
              " 'happi end awesom act actress',\n",
              " 'ador truli great experi',\n",
              " 'bad upto hype',\n",
              " 'could better',\n",
              " 'sure disapoint meal']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In this portion, We are doing Vectorization"
      ],
      "metadata": {
        "id": "M9yoKz68sKuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer "
      ],
      "metadata": {
        "id": "WAeczZ5Or3mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are creating the instance of the CountVectorizer and setting the ngram range\n",
        "cv = CountVectorizer(ngram_range=(1,2))"
      ],
      "metadata": {
        "id": "uuwRFFjXtDx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are vectozing our input and converting it into an array\n",
        "X_vec = cv.fit_transform(X_clean).toarray()"
      ],
      "metadata": {
        "id": "31KhnH2HtOpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJWOoiJeto71",
        "outputId": "a3870dc7-1251-4cb0-cc7f-a1b5d494f057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this count vectorizer tells us how many times the feature manes has repeated in the array of X_vec\n",
        "this model can also be reffered as a bag of words model"
      ],
      "metadata": {
        "id": "7vlUA-Ay2Yp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5-snx-Mtq4t",
        "outputId": "61616fed-b9cc-4fe5-87e7-703f9185d327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['act' 'act actress' 'actress' 'ador' 'ador truli' 'awesom' 'awesom act'\n",
            " 'awesom tasti' 'bad' 'bad upto' 'better' 'could' 'could better'\n",
            " 'disapoint' 'disapoint meal' 'end' 'end awesom' 'experi' 'food'\n",
            " 'food like' 'great' 'great experi' 'great food' 'happi' 'happi end'\n",
            " 'hype' 'like' 'like lot' 'lot' 'meal' 'sure' 'sure disapoint' 'tasti'\n",
            " 'tasti food' 'truli' 'truli great' 'upto' 'upto hype']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xt_vect = cv.transform(Xt_clean).toarray()"
      ],
      "metadata": {
        "id": "RSZvdBfoxqHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xt_vect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXLK2YRn1AB2",
        "outputId": "d36d1cff-dbbb-468d-efc0-0ff8e77eb919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes\n",
        "for this sentiment analysis we are using Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "XHBMYNhC1HRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#we are creating an instance of that\n",
        "mn = MultinomialNB()\n",
        "#we  fit our model\n",
        "mn.fit(X_vec, Y_train)\n",
        "#the output in the multinomial naive bayes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tEsRplC1VDw",
        "outputId": "3c0d3c06-f17c-4e90-d9aa-50e095378227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are performing our prediction"
      ],
      "metadata": {
        "id": "-Br1x_6p4ICz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are passing the test vector value to make a prediction\n",
        "y_pred = mn.predict(Xt_vect)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMMMy2d92s6_",
        "outputId": "2b60d481-4e75-465d-955f-65aee273c835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here in the Output y_pred, we see that the value in the array is [1,0].\n",
        "\n",
        "If we look at our testing data, we cound see that the first sentence is a positive sentence. As it can be seen on the first value of the array is 1. \n",
        "\n",
        "The second sentence is a negative sentence in the testing data.  s it can be seen on the second value of the array is 0.\n",
        "\n",
        "As we defined in our dataset, Positive -> 1, negative -> 0\n",
        "\n",
        "So, we can see that our classification is working. "
      ],
      "metadata": {
        "id": "u14picWk5Ngv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wVir8t6s2-uY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}